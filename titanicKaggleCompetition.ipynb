{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Kaggle Titanic competition\n",
    "## Coursera Data Science at Scale assignment\n",
    "\n",
    "### Problem description\n",
    "\n",
    "In the Kaggle Titanic competition we are asked to analyse which sorts of people did survive the well known Titanic catastrophe. As ususal with a Kaggle competition, the data is already split into train and test set. Apart from a unique identifier for the passenger *PassengerId* and the target class *Survived* we have ten attributes at our hand:\n",
    "\n",
    "**Attribute**|**values**\n",
    "--------|--------------\n",
    "Pclass | 1, 2 or 3\n",
    "Name | String\n",
    "Sex | male or female\n",
    "Age | int > 0\n",
    "SibSp | int \n",
    "Parch | int\n",
    "Ticket | Ticket code; string\n",
    "Fare | float\n",
    "Cabin | String\n",
    "Embarked | C = Cherbourg; Q = Queenstown; S = Southampton\n",
    "\n",
    "To get a good overview of how the attributes relate to each other, you can look at the seaborn pairplot example by Ben Hammer: https://www.kaggle.io/svf/1603/1db145f4c65249a8bc7b7090fb66369b/1_seaborn_pair_plot.png and also his importance analysis plot: https://www.kaggle.io/svf/1495/cd66e2a313cb459c8191405df5226dc9/2_feature_importance.png\n",
    "\n",
    "The results of the Kaggle competition will be evaluated based on the percentage of correctly predicted outcomes (*Survived* = 0 or 1) for all passengers in the test set. The Kaggle public leader board only shows the score on 50% of the test set (random). The remaining 50% are used for a private leader board to determine the final winner and prevent overfitting.\n",
    "\n",
    "The submission format is a two column format: PassengerId, Survived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Approach\n",
    "How I approached the problem: I decided to create a KNIME workflow to solve the problem. \n",
    "\n",
    "Overall dataset size: train: 891 passengers, test: 418 passengers\n",
    "\n",
    "My first approach to get a feeling for the problem and the dataset is to do a little manual feature selection (there are obviously features without any use here), data cleaning and then train a baseline random forest model. This should give a first hint how hard/easy it is to get a good performing model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Solution\n",
    "First of all I did a manual feature selection and removed attributes PassengerId, Name and Ticket as those should defenitely have no influence on the survival of a passenger. I also did remove the Cabin attribute (at least for now) as there are a lot of missing values in this column (train: , test: 327).\n",
    "As a second preprocessing step I handled missing values:\n",
    "\n",
    "Attribute | missing value strategy | missing values train | missing values test set\n",
    "--------- | ---------------------- | -------------------- | ----------------------\n",
    "Age | rounded mean | 177 | 86\n",
    "Fare | mean |  0 | 1\n",
    "Embarked | most frequent value | 2 |  0\n",
    "\n",
    "This was done for training and test set in analogy.\n",
    "\n",
    "For the initial baseline model I used the Weka RandomForest node to train a model on the complete train set with default parameters (Weka command line: -I 10 -K 0 -S 1 -num-slots 1; this means no limit on the number of features, no limit on the depth of the trees, 10 trees to be generated). The predictions were then printed to a file of the requested submission format and submitted to Kaggle.\n",
    "\n",
    "Solution details:\n",
    "I used the following KNIME workflow nodes: FileReader, Statistics, Column Filter, Missing Value, Number To String, RandomForest (Weka3.7), WekaPredictor(Weka3.7), Column Filter, CSV Writer\n",
    "\n",
    "![](baseline.PNG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Solution Analysis\n",
    "The initial solution showed an accuracy of 0.76077. In comparison to the gender based model (0.76555) this is roughly in the same region but not yet a satisfying solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revised Solution and Analysis\n",
    "As the baseline model is gender based, I thought that maybe it might also be a good idea to do a split by passenger class instead of gender. So I split the train and test data into three parts (by Pclass) and trained a random forest model (same parameters as in the baseline model) for each of the three and used the models for prediction. I submitted the concatenated solutions as one solution to Kaggle. \n",
    "\n",
    "Unfortunately, the reported accuracy in the leaderboard did not improve. Actually it went down to 0.74641. Changing the number of trees parameter from 10 to 5 did further decrease the performance to 0.73684. Changing it to 15 resulted in an accuracy of 0.73684, too."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
